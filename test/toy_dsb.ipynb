{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Tuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch.version\n",
    "\n",
    "\n",
    "# print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=128, output_dim=2, T = 100):\n",
    "        \"\"\"\n",
    "        input_dim: dimension of input (x and t concatenated)\n",
    "        output_dim: dimension of output (drift term)\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim + 32, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "        self.time_emb = nn.Embedding(T, 32)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, 2)\n",
    "        t: Tensor of shape (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # Concatenate x and t\n",
    "        # print(x.shape, self.time_emb(t).shape)\n",
    "        xt = torch.cat([x, self.time_emb(t)], dim=-1)  # Shape: (batch_size, 3)\n",
    "        return self.net(xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SchrödingerBridgeDataset(Dataset):\n",
    "    def __init__(self, initial_samples: int, final_samples: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            initial_samples (int): Number of samples from the initial distribution.\n",
    "            final_samples (int): Number of samples from the final distribution.\n",
    "        \"\"\"\n",
    "        super(SchrödingerBridgeDataset, self).__init__()\n",
    "        \n",
    "        self.initial_data = self.uniform_distribution(final_samples)\n",
    "        # self.final_data = self.uniform_distribution(initial_samples) + 2\n",
    "        self.final_data = self.bimodal_distribution(final_samples) + 2\n",
    "        random_index = torch.randperm(len(self.final_data))\n",
    "        self.final_data = self.final_data[random_index, :]\n",
    "    @staticmethod\n",
    "    def uniform_distribution(batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Uniform distribution in [-1, 1] x [-1, 1].\n",
    "        \"\"\"\n",
    "        return torch.rand(batch_size, 1) * 2 - 1  # Shape: (batch_size, 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def bimodal_distribution(batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Bimodal distribution with two Gaussian clusters.\n",
    "        \"\"\"\n",
    "        half = batch_size // 2\n",
    "        cluster1 = torch.randn(half, 1) + 2.0  # Cluster centered at (2)\n",
    "        cluster2 = torch.randn(batch_size - half, 1) - 2.0  # Cluster centered at (-2)\n",
    "        return torch.cat([cluster1, cluster2], dim=0)  # Shape: (batch_size, 1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Total samples are from both initial and final distributions\n",
    "        return max(len(self.initial_data), len(self.final_data))\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns a tuple of (x_initial, x_final).\n",
    "        If one distribution has fewer samples, wrap around using modulo.\n",
    "        \"\"\"\n",
    "        x_initial = self.initial_data[idx % len(self.initial_data)]\n",
    "        x_final = self.final_data[idx % len(self.final_data)]\n",
    "        return {\"x0\": x_initial, \"xT\":x_final}\n",
    "    def plot(self):\n",
    "        bins = np.linspace(-5, 5, 100)\n",
    "        plt.hist(self.initial_data.detach().numpy(), bins = bins, alpha = 0.3, label = \"inital_distribution\")\n",
    "        plt.hist(self.final_data.detach().numpy(), bins = bins, alpha = 0.3, label = \"final_distribution\")\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = SchrödingerBridgeDataset(10000, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ds[0])\n",
    "ds.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lagevin(x:torch.Tensor, model:nn.Module, t:torch.IntTensor, T:int, gamma:torch.Tensor):\n",
    "    traj_x = [x.unsqueeze(-1)]\n",
    "    for i in range(0, T):\n",
    "        time = torch.full((x.shape[0], 1), fill_value = i)\n",
    "        x = x + gamma*model(x, time) + torch.sqrt(2*gamma)*torch.rand_like(x)\n",
    "        traj_x.append(x.unsqueeze(-1))\n",
    "    all_x = torch.cat(traj_x, dim = -1)\n",
    "    return torch.gather(all_x, dim = -1, index = t)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_process(x0, xT, T, sigma, inverse = False):\n",
    "    x_all = torch.zeros((x0.shape[0], x0.shape[1], T+1))\n",
    "    grad_all = torch.zeros((x0.shape[0], x0.shape[1], T))\n",
    "    x_all[:,:,0] = x0\n",
    "    for i in range(1,T+1):\n",
    "        mu_i = (xT - x0)*i/T\n",
    "        x_all[:, :, i] = x0 + mu_i + sigma[i-1]*torch.randn_like(x0)\n",
    "        grad_all[:,:,i-1] = x_all[:,:,i] - x_all[:,:,i-1] if not inverse else -x_all[:,:,i] + x_all[:,:,i-1]\n",
    "    return x_all, grad_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds.final_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all, grad_all = first_process(ds.final_data, ds.initial_data, T = 50, sigma=1e-2*torch.ones((len(ds.final_data)))*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(grad_all.shape)\n",
    "# plt.hist(ds.final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ds.final_data.numpy(), alpha = 0.1)\n",
    "plt.hist(x_all[:, 0,0], alpha = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_all.numpy()[:, 0, 0].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_all.numpy()[-50:, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_all.shape)\n",
    "# bins = np.linspace(-5, 5, 100)\n",
    "for i in range(0, 100):\n",
    "    # i = np.random.randint(0, 1000)\n",
    "    plt.plot(grad_all.numpy()[i, 0, :], alpha = 0.3, color = \"#123499\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_dsb(dataset, batch_size, epoches, f_model, b_model,f_optim, b_optim, sigma, T, device):\n",
    "    dl = DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "    pbar = tqdm(range(epoches), desc=f\"Epoch\", leave=False)\n",
    "    total_loss_f = []\n",
    "    total_loss_b = []\n",
    "    f_model.train()\n",
    "    b_model.eval()\n",
    "    for inner_epoch in pbar:\n",
    "        epoch_loss_b = 0\n",
    "        for data in dl:\n",
    "            x0, xT = data['x0'].to(device), data[\"xT\"].to(device)\n",
    "            x_all, grad_all = first_process(x0, xT, T = T, sigma=sigma)\n",
    "            x_all = x_all[..., :-1]\n",
    "            sample_t = torch.randint(high = T-2, low = 0, dtype= torch.int64, size = (x0.shape[0], 1, 1))\n",
    "            sample_xk = torch.gather(x_all, dim = -1, index = sample_t)\n",
    "            grad_out = torch.gather(grad_all, dim = -1, index = sample_t)\n",
    "            pred_out = f_model(sample_xk, sample_t.squeeze(-1))\n",
    "            f_optim.zero_grad()\n",
    "            loss_b = nn.functional.mse_loss(input = pred_out, target= grad_out)\n",
    "            loss_b.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(f_model.parameters(), 1)\n",
    "            f_optim.step()\n",
    "            epoch_loss_b += loss_b.item()\n",
    "        pbar.set_postfix({\"forward_loss\": epoch_loss_b/len(dl)})\n",
    "        total_loss_b.append(epoch_loss_b/len(dl))\n",
    "    plt.plot(total_loss_b)\n",
    "    plt.show()\n",
    "    return f_model, b_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 200\n",
    "f_model = MLP(1, 256, 1, T)\n",
    "b_model = MLP(1, 256, 1, T)\n",
    "f_optim = torch.optim.Adam(f_model.parameters())\n",
    "b_optim = torch.optim.Adam(b_model.parameters())\n",
    "device = \"cpu\"\n",
    "sigma = torch.from_numpy(np.linspace(1, 0, T)*1e-1).unsqueeze(-1)\n",
    "# print(sigma.shape)\n",
    "simple_dsb(ds, 100, 200, f_model = f_model, b_model = b_model, f_optim = f_optim, b_optim = b_optim, sigma = sigma, T = T, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_model(torch.ones((T,1,1))*2, torch.arange(0, T).unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, T, dataset, sigma, fb = \"f\"):\n",
    "    dl = DataLoader(dataset=dataset, batch_size=len(dataset))\n",
    "    for data in dl:\n",
    "        # Make forward process \n",
    "        # print(data)\n",
    "        x0, xT = data['x0'].to(device), data[\"xT\"].to(device)\n",
    "        xk = x0 if fb == 'f' else xT\n",
    "        x_all = [xk.unsqueeze(-1)]\n",
    "        time_step = [i for i in range(0, T)]\n",
    "        time_step = time_step if fb == \"f\" else reversed(time_step)\n",
    "        for i in time_step:\n",
    "            # print(i)\n",
    "            xk = xk + model(xk, i*torch.ones((xk.shape[0]), dtype=torch.int32)) + sigma[i]*torch.randn_like(xk)\n",
    "            x_all.append(xk.unsqueeze(-1))\n",
    "    x_all = torch.cat(x_all, axis = -1)\n",
    "    return x_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sigma.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all = sample(model= f_model, T = T, dataset = ds, sigma = sigma.to(torch.float32), fb = \"f\")\n",
    "x_all = x_all.detach().numpy()\n",
    "print(x_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(x_all[:,:,0], bins = np.linspace(-10, 10, 100), alpha = 0.3)\n",
    "# # plt.hist(x_all[:,:,1], bins = np.linspace(-10, 10, 100), alpha = 0.3)\n",
    "# # plt.hist(x_all[:,:,2], bins = np.linspace(-10, 10, 100), alpha = 0.3)\n",
    "# plt.hist(x_all[:,:,-1], bins = np.linspace(-10, 10, 100), alpha = 0.3)\n",
    "# plt.show()\n",
    "for i in range(len(x_all)):\n",
    "# i = np.random.randint(0, len(x_all))\n",
    "    plt.plot(x_all[i, 0, :], color = \"#123499\", alpha = 0.3)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.linspace(-10,10,50)\n",
    "plt.hist(x_all[:, 0, 0], bins = bins, alpha = 0.3, label = \"initial_distribution\", density = True)\n",
    "plt.hist(x_all[:, 0, -1], bins = bins, alpha = 0.3, label = \"generated_distribution\", density = True)\n",
    "plt.hist(ds.final_data.numpy(), bins = bins, alpha = 0.3, label = \"final_distribution\", density = True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.hist(np.random.standard_normal((x_all.shape[0],)), bins = bins, alpha = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsb(dataset, batch_size, epoches, f_model, b_model,f_optim, b_optim, sigma, T, device):\n",
    "    dl = DataLoader(dataset=dataset, batch_size=batch_size)\n",
    "    pbar = tqdm(range(epoches), desc=f\"Epoch\", leave=False)\n",
    "    total_loss_f = []\n",
    "    total_loss_b = []\n",
    "    for epoch in pbar:\n",
    "        \n",
    "        loss_f_all = 0\n",
    "        loss_b_all = 0\n",
    "        for inner_epoch in range(0, 1):\n",
    "            f_model.train()\n",
    "            b_model.eval()\n",
    "            for data in dl:\n",
    "                # Make backward process\n",
    "                x0, xT = data['x0'].to(device), data[\"xT\"].to(device)\n",
    "                px_all = torch.zeros((xT.shape[0], xT.shape[1], T))\n",
    "                xk = xT\n",
    "                # qx_all[:, :, 0] = xk\n",
    "                with torch.no_grad():\n",
    "                    for i in range(T-1, -1, -1):\n",
    "                        # print(i)\n",
    "                        xk = xk + b_model(xk, i*torch.ones((xk.shape[0]), dtype=torch.int32))*sigma[i] + torch.sqrt((2*sigma[i]))*torch.randn_like(xk)\n",
    "                        px_all[:, :, i] = xk\n",
    "                sample_t = torch.randint(high = T-2, low = 0, dtype= torch.int64, size = (x0.shape[0], 1, 1))\n",
    "                sample_tp1= sample_t + 1\n",
    "                \n",
    "                # sample_tp1[sample_tp1 == T] =T\n",
    "                sample_xk = torch.gather(px_all, dim = -1, index = sample_t)\n",
    "                sample_xk1 = torch.gather(px_all, dim = -1, index = sample_tp1)\n",
    "                sample_t, sample_tp1 = sample_t.view(x0.shape[0]), sample_tp1.view(x0.shape[0])\n",
    "                \n",
    "                B_kp1_xkp1 = sample_xk1 + sigma[sample_tp1] * b_model(sample_xk1.squeeze(-1), sample_tp1)\n",
    "                B_kp1 = sample_xk1 + sigma[sample_tp1] * b_model(sample_xk.squeeze(-1), sample_tp1)\n",
    "                F_k = sample_xk + sigma[sample_tp1] * f_model(sample_xk.squeeze(-1), sample_t)\n",
    "                f_optim.zero_grad()\n",
    "                loss_f = nn.functional.mse_loss(input = F_k, target=(sample_xk.squeeze(-1) + B_kp1_xkp1 - B_kp1))\n",
    "                loss_f.backward()\n",
    "                # torch.nn.utils.clip_grad_norm_(f_model.parameters(), 1)\n",
    "                f_optim.step()\n",
    "                loss_f_all += loss_f.item()\n",
    "        for inner_epoch in range(0, 1):\n",
    "            loss_b_all = 0\n",
    "            b_model.train()\n",
    "            f_model.eval()\n",
    "            for data in dl:\n",
    "                # Make forward process \n",
    "                # print(data)\n",
    "                x0, xT = data['x0'].to(device), data[\"xT\"].to(device)\n",
    "                qx_all = torch.zeros((x0.shape[0], x0.shape[1], T))\n",
    "                # qx_all.append(xT)\n",
    "                # print(x0.shape)\n",
    "                # px_all.append(x0)\n",
    "                xk = x0\n",
    "                # px_all[:, :, 0] = xk\n",
    "                for i in range(0, T):\n",
    "                    xk = xk + f_model(xk, i*torch.ones((xk.shape[0]), dtype=torch.int32))*sigma[i] + torch.sqrt((2*sigma[i]))*torch.randn_like(xk)\n",
    "                    qx_all[:, :, i] = xk\n",
    "                \n",
    "                sample_t = torch.randint(high = T-2, low = 0, dtype= torch.int64, size = (x0.shape[0], 1, 1))\n",
    "                sample_tp1= sample_t + 1\n",
    "                \n",
    "                \n",
    "                # Train Backward process:\n",
    "                \n",
    "                sample_t = torch.randint(high = T-2, low = 0, dtype= torch.int64, size = (x0.shape[0], 1, 1))\n",
    "                sample_tp1= sample_t + 1\n",
    "                # print(qx_all.shape, sample_t.shape)\n",
    "                # sample_tp1[sample_tp1 == T] =T\n",
    "                sample_xk = torch.gather(qx_all, dim = -1, index = sample_t)\n",
    "                sample_xk1 = torch.gather(qx_all, dim = -1, index = sample_t)\n",
    "                sample_t, sample_tp1 = sample_t.view(x0.shape[0]), sample_tp1.view(x0.shape[0])\n",
    "                B_kp1 = sample_xk1 + b_model(sample_xk1.squeeze(-1), sample_tp1)*sigma[sample_tp1]\n",
    "                F_k = sample_xk + sigma[sample_tp1] * f_model(sample_xk.squeeze(-1), sample_t)\n",
    "                F_kp1 = sample_xk1 + sigma[sample_tp1] * f_model(sample_xk1.squeeze(-1), sample_t)\n",
    "                b_optim.zero_grad()\n",
    "                loss_b = nn.functional.mse_loss(input = B_kp1, target= (sample_xk1.squeeze(-1) + F_k - F_kp1))\n",
    "                loss_b.backward()\n",
    "                # torch.nn.utils.clip_grad_norm_(b_model.parameters(), 1)\n",
    "                b_optim.step()\n",
    "                loss_b_all += loss_b.item()\n",
    "        \n",
    "        \n",
    "            total_loss_f.append(loss_f_all/len(dl))\n",
    "            total_loss_b.append(loss_b_all/len(dl))\n",
    "            pbar.set_postfix({\"forward_loss\": loss_f_all/len(dl), \"backward_loss\": loss_b_all/len(dl)})\n",
    "\n",
    "    plt.plot(total_loss_f, label = \"forward loss\")\n",
    "    plt.plot(total_loss_b, label = \"backward loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return f_model, b_model\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_model = MLP(1, 128, 1)\n",
    "b_model = MLP(1, 128, 1)\n",
    "f_optim = torch.optim.Adam(f_model.parameters())\n",
    "b_optim = torch.optim.Adam(b_model.parameters())\n",
    "device = \"cpu\"\n",
    "T = 5\n",
    "dsb(ds, 100, 50, f_model = f_model, b_model = b_model, f_optim = f_optim, b_optim = b_optim, sigma = torch.ones((T,1))*1e-2, T = T, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, T, dataset, sigma, fb = \"f\"):\n",
    "    dl = DataLoader(dataset=dataset, batch_size=len(dataset))\n",
    "    for data in dl:\n",
    "        # Make forward process \n",
    "        # print(data)\n",
    "        x0, xT = data['x0'].to(device), data[\"xT\"].to(device)\n",
    "        xk = x0 if fb == 'f' else xT\n",
    "        x_all = [xk.unsqueeze(-1)]\n",
    "        time_step = [i for i in range(0, T)]\n",
    "        time_step = time_step if fb == \"f\" else reversed(time_step)\n",
    "        for i in time_step:\n",
    "            # print(i)\n",
    "            xk = xk + model(xk, i*torch.ones((xk.shape[0]), dtype=torch.int32))*sigma[i] + 2*sigma[i]*torch.randn_like(xk)\n",
    "            x_all.append(xk.unsqueeze(-1))\n",
    "    x_all = torch.cat(x_all, axis = -1)\n",
    "    return x_all\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_all = sample(model= f_model, T = T, dataset = ds, sigma = torch.ones((T,1))*1e-2, fb = \"f\")\n",
    "print(T)\n",
    "x_all = sample(model= f_model, T = T, dataset = ds, sigma = torch.ones((T,1))*1e-2, fb = \"f\")\n",
    "x_all = x_all.detach().numpy()\n",
    "print(x_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_all = x_all.detach().numpy()\n",
    "plt.hist(x_all[:,:,0], bins = np.linspace(-10, 10, 100), alpha = 0.3)\n",
    "plt.hist(x_all[:,:,1], bins = np.linspace(-10, 10, 100), alpha = 0.3)\n",
    "plt.hist(x_all[:,:,2], bins = np.linspace(-10, 10, 100), alpha = 0.3)\n",
    "plt.hist(x_all[:,:,-1], bins = np.linspace(-10, 10, 100), alpha = 0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(0, 1000):\n",
    "    # print(x_all[i, :, :].shape)\n",
    "    plt.plot(x_all[i, 0, :], color = \"#123499\", alpha = 0.1)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Trainer:\n",
    "#     def __init__(self,\n",
    "#                  forward_model: nn.Module,\n",
    "#                  backward_model: nn.Module,\n",
    "#                  forward_optimizer: optim.Optimizer,\n",
    "#                  backward_optimizer: optim.Optimizer,\n",
    "#                  device: torch.device,\n",
    "#                  dataset: Dataset,\n",
    "#                  n_steps: int = 1000,\n",
    "#                  batch_size: int = 256,\n",
    "#                  T: float = 1.0,\n",
    "#                  time_steps: int = 100):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             forward_model (nn.Module): Model for the forward process.\n",
    "#             backward_model (nn.Module): Model for the backward process.\n",
    "#             forward_optimizer (optim.Optimizer): Optimizer for forward model.\n",
    "#             backward_optimizer (optim.Optimizer): Optimizer for backward model.\n",
    "#             device (torch.device): Device to run the training on.\n",
    "#             dataset (Dataset): Dataset providing (x_initial, x_final) pairs.\n",
    "#             n_steps (int): Number of training steps.\n",
    "#             batch_size (int): Batch size.\n",
    "#             T (float): Total time.\n",
    "#             time_steps (int): Number of time discretization steps.\n",
    "#         \"\"\"\n",
    "#         self.forward_model = forward_model.to(device)\n",
    "#         self.backward_model = backward_model.to(device)\n",
    "#         self.forward_optimizer = forward_optimizer\n",
    "#         self.backward_optimizer = backward_optimizer\n",
    "#         self.device = device\n",
    "#         self.n_steps = n_steps\n",
    "#         self.batch_size = batch_size\n",
    "#         self.T = T\n",
    "#         self.time_steps = time_steps\n",
    "#         self.dt = self.T / self.time_steps\n",
    "        \n",
    "#         # DataLoader\n",
    "#         self.loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True, drop_last=True)\n",
    "    \n",
    "#     def train_step_forward(self, x0: torch.Tensor, t: torch.Tensor, xT: torch.Tensor) -> float:\n",
    "#         \"\"\"\n",
    "#         Performs a single training step for the forward model.\n",
    "        \n",
    "#         Args:\n",
    "#             x0 (torch.Tensor): Initial states.\n",
    "#             t (torch.Tensor): Time scalars.\n",
    "#             xT (torch.Tensor): Final states.\n",
    "        \n",
    "#         Returns:\n",
    "#             float: Loss value.\n",
    "#         \"\"\"\n",
    "#         self.forward_optimizer.zero_grad()\n",
    "        \n",
    "#         # Predict drift at (x0, t)\n",
    "#         drift_forward = self.forward_model(x0, t)\n",
    "        \n",
    "#         # Placeholder loss: MSE between predicted drift and desired drift\n",
    "#         # In practice, replace with appropriate Schrödinger bridge loss\n",
    "#         desired_drift_forward = (xT - x0) / self.T\n",
    "#         loss_forward = nn.MSELoss()(drift_forward, desired_drift_forward)\n",
    "        \n",
    "#         loss_forward.backward()\n",
    "#         self.forward_optimizer.step()\n",
    "        \n",
    "#         return loss_forward.item()\n",
    "    \n",
    "#     def train_step_backward(self, xT: torch.Tensor, t: torch.Tensor, x0: torch.Tensor) -> float:\n",
    "#         \"\"\"\n",
    "#         Performs a single training step for the backward model.\n",
    "        \n",
    "#         Args:\n",
    "#             xT (torch.Tensor): Final states.\n",
    "#             t (torch.Tensor): Time scalars.\n",
    "#             x0 (torch.Tensor): Initial states.\n",
    "        \n",
    "#         Returns:\n",
    "#             float: Loss value.\n",
    "#         \"\"\"\n",
    "#         self.backward_optimizer.zero_grad()\n",
    "        \n",
    "#         # Predict drift at (xT, T - t)\n",
    "#         # Note: For backward model, time is reversed\n",
    "#         t_backward = self.T - t\n",
    "#         drift_backward = self.backward_model(xT, t_backward)\n",
    "        \n",
    "#         # Placeholder loss: MSE between predicted drift and desired drift\n",
    "#         desired_drift_backward = (x0 - xT) / self.T\n",
    "#         loss_backward = nn.MSELoss()(drift_backward, desired_drift_backward)\n",
    "        \n",
    "#         loss_backward.backward()\n",
    "#         self.backward_optimizer.step()\n",
    "        \n",
    "#         return loss_backward.item()\n",
    "    \n",
    "#     def train(self):\n",
    "#         \"\"\"\n",
    "#         Runs the training loop for both forward and backward models.\n",
    "#         \"\"\"\n",
    "#         for step in range(1, self.n_steps + 1):\n",
    "#             try:\n",
    "#                 x0, xT = next(self.data_iter)\n",
    "#             except AttributeError:\n",
    "#                 self.data_iter = iter(self.loader)\n",
    "#                 x0, xT = next(self.data_iter)\n",
    "            \n",
    "#             x0 = x0.to(self.device)\n",
    "#             xT = xT.to(self.device)\n",
    "            \n",
    "#             # Sample time uniformly from [0, T]\n",
    "#             t = torch.rand(x0.size(0), 1).to(self.device) * self.T  # Shape: (batch_size, 1)\n",
    "            \n",
    "#             # Forward step\n",
    "#             loss_forward = self.train_step_forward(x0, t, xT)\n",
    "            \n",
    "#             # Backward step\n",
    "#             loss_backward = self.train_step_backward(xT, t, x0)\n",
    "            \n",
    "#             if step % 100 == 0 or step == 1:\n",
    "#                 print(f\"Step {step}/{self.n_steps} | Forward Loss: {loss_forward:.4f} | Backward Loss: {loss_backward:.4f}\")\n",
    "        \n",
    "#         print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bridge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
